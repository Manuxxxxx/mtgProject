# MTG Commander Synergy & Tag Prediction System (Draft)

## Introduction & Objectives
The project aims to build a recommender system that suggests cards to add to a partially completed Commander deck in Magic: The Gathering (MTG). Commander decks require exactly 100 singleton cards, and synergy is highly contextual. 

The motivation is to recommend cards based on **actual utility** instead of statistics derived from other players’ decks (as in EDHREC). In theory, this allows the system to suggest **unconventional but effective cards** that statistical methods might overlook.

Goals:
- Predict functional tags for cards.
- Predict synergies between card pairs.
- Support deck-building recommendations that go beyond archetype bias.
- Explore modular architectures and toggles (detach, symmetry, hidden-layer vs projector, multitask projector).
- Investigate penalties for tag hierarchies (parent-child consistency).

A key limitation is that the current approach only considers **pairs of cards**. It cannot yet capture **multi-card combos**, which are central to MTG but more complex.

---

## Data Sources

### Functional Tags
- Scraped from ScryfallTagger (community-assigned labels such as “activated ability,” “animate artifact”).
- Cards typically have 6–7 tags when including parent tags.
- Total vocabulary: ~3700+ tags, though most are rare/unconventional. Training experiments used the most frequent 130 or 199 tags.
- Tag vocabulary evolves over time; goal: capture finer functionality without excessive complexity.

### Synergy Data
- Scraped from EDHREC, but two main limitations:
  1. EDHREC’s “synergy” is based on deck archetype statistics, not strict commander–card relations.
  2. Thresholds may not be optimal (synergy > 0.4 → label 1; synergy < 0.2, possibly negative → label 0).
- Negative examples generated with random card pairs (weak negatives).

**Types of relationships defined:**
- **True synergy**: two cards directly improve each other (e.g., “create a token” + “sacrifice a token to draw”).
- **Archetype synergy**: cards fit into a broader deck type (e.g., artifact archetype) without direct commander synergy.
- **Similarity**: cards with comparable effects, useful for redundancy but not synergistic (e.g., two cards that both sacrifice tokens for minor effects).

### Custom GUI Tool
- Supports manual labeling of synergies (currently ~100 samples, not yet included in training).
- GUI displays card info and enables two label types on [-1,1] scale (step=0.25):
  - **Synergy**
  - **Similarity**

### Dataset Setup
- Full set of MTG cards.
- Split into training/validation/test sets.
- Tag thresholds (min count 50).

---

## Models & Architectures

### BERT/DistilBERT/DeBERTa
- Embedding dimension fixed at 384 (GTX 970, 4GB VRAM).
- Tested BERT-base, DistilBERT, and DeBERTa v3 small (no major differences).

### Tag Model
- Input: BERT embeddings.
- Output: multi-label functional tags.
- Loss: Binary Cross-Entropy (BCE) or Focal Loss with class balancing.
- Architectures:
  - [512 → 256 → output] or [512 → 512 → output].
- Hierarchical penalty (parent-child consistency). Weighting changes plateaued due to strong tag dependencies.
- Attempted attention block → no significant improvement (40+ epochs).

### Tag Projector (Optional)
- Projects predicted tag vectors into dense embeddings for synergy model.
- Alternative: **Tag Hidden Layer Integration** → use hidden-layer activations instead.
- Important: **mutually exclusive** (must use either projector OR hidden-layer embeddings).
- Detach option: block gradient flow into Tag Model.

### Synergy Model
- Inputs: BERT embeddings + either projected tags OR hidden-layer embeddings, combined with element-wise interactions (concat, product, abs-diff).
- Output: binary synergy prediction.
- Loss: cross-entropy.
- Architectures:
  - Symmetrical (order-invariant) or Non-symmetrical (order-sensitive).
- Re-implemented symmetry → no major changes.

### MultiTask Projector (Optional)
- Splits embeddings into tag and synergy heads.
- Implemented as two-head MLP.
- Promising but costly → slower convergence, high compute cost.

---

## Training Setup
- Multiple learning rates:
  - Lower for pretrained BERT.
  - Higher for custom heads (tag/synergy).
  - Optional LR for projectors.
- Loss scaling: synergy loss dominates → custom LossScaler required.
- Training phases:
  1. Train Tag Model standalone.
  2. Joint training with Synergy Model (detach often enabled).
- Observations:
  - No overfitting observed (validation/test never decreased).
  - Batch size ~5 with gradient accumulation.
  - 1 epoch ~30+ min for Tag Model.
- Each run controlled by JSON configs (hyperparameters, toggles).

---

## Experiments & Results
### Tag Model
- Trained on 130 vs 199 most frequent tags (with and without hierarchy/penalty).
- Similar progression; larger tag set slower to converge.
- Performance worsens with more tags due to:
  - Parent–child dependency complexity.
  - More output classes → harder learning.
  - Small embedding dimension (384).

### Synergy Model
- Decent F1, but dataset insufficient.
- Overfits legendary synergies (EDHREC bias).
- Predicts similarity/archetype effects rather than true synergy (e.g., haste + flying).
- Symmetry fix → no major differences.
- Limitations: dataset size, weak negatives, EDHREC threshold bias.

### Joint Training
- Without balancing: synergy loss dominated, hurting tag model.
- With LossScaler: both models trained correctly.
- Detach toggle → prevented synergy gradients overwriting tag learning.
- MultiHead projector: promising but costly.
- Tag Projector: no clear gains.

---

## Limitations & Challenges
- Hardware: GTX 970, 4GB VRAM → very slow, small batch size.
- Negative samples from random pairs introduce bias.
- Tag projector vs hidden-layer effects unclear.
- Difficult to capture multi-card combos (pair-based only).
- Expanded tag vocabulary causes dimensional instability.
- Rare tags + parent-child dependencies degrade performance.

---

## Current Goals
- Compare hidden-layer embeddings vs projected tags.
- Measure detach effects on joint training.
- Evaluate symmetry importance.
- Test MultiTaskProjector more fully.
- Expand tag vocabulary while managing complexity.
- Collect larger labeled synergy dataset (replace EDHREC negatives).

---

## Future Work
- Collect more synergy labels via GUI/community.
- Explore dual encoders, contrastive training, graph-based methods.
- Full MultiTaskProjector runs.
- Experiment with schedulers, warmup, larger models.
- Better negative sampling.
- Explore tag compression (hierarchy grouping, autoencoder).

---

## Notes & Tools
- Exclusivity enforced: projector vs hidden-layer.
- Detach and Symmetry toggles in configs.
- Precompute/cache tag embeddings to save GPU.
- Designed comparison experiments (hidden-layer vs projector, detach vs joint, symmetry vs order, multitask vs shared).
- Visualization tools:
  - Script saves computed synergies to SQLite DB.
  - Local webapp visualizes synergy graph (1 set recommended to avoid overload).
